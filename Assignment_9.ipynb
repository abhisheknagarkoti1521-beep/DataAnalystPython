{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#ASSIGNMENT 9\n",
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "\n",
        "**Information Gain** is a metric used in **Decision Tree algorithms** to decide which attribute should be selected to split the data at each node. It measures how much **uncertainty or impurity** in the dataset is reduced after splitting the data based on a particular feature.\n",
        "\n",
        "In decision trees, impurity is measured using **Entropy**. Entropy shows how mixed the data is. If all records belong to the same class, entropy is zero, which means there is no uncertainty. When data is evenly mixed, entropy is maximum.\n",
        "\n",
        "The formula for entropy is:\n",
        "Entropy(S)=−∑pi​log2​pi​\n",
        "\n",
        "where ( p_i ) is the probability of each class.\n",
        "\n",
        "Information Gain is calculated as the difference between the entropy of the parent node and the weighted average entropy of the child nodes after the split.\n",
        "\n",
        "Information Gain=Entropy(Parent)−∑∣Parent∣∣Child∣​×Entropy(Child)\n",
        "\n",
        "**Use of Information Gain in Decision Trees:**\n",
        "\n",
        "1. Initially, the entropy of the complete dataset is calculated.\n",
        "2. For each attribute, the dataset is split based on its possible values.\n",
        "3. Entropy is calculated for each subset formed after the split.\n",
        "4. Information Gain is computed for every attribute.\n",
        "5. The attribute with the **highest Information Gain** is selected as the decision node.\n",
        "6. This process is repeated recursively for each child node until the tree is fully constructed or stopping conditions are met.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Helps in selecting the most relevant attribute.\n",
        "* Reduces uncertainty effectively.\n",
        "* Improves classification accuracy.\n",
        "\n",
        "**Example:**\n",
        "If a dataset is split using the attribute **“Outlook”** and it results in a higher reduction of entropy compared to other attributes, then “Outlook” will be chosen as the root node of the decision tree.\n",
        "\n",
        "**Conclusion:**\n",
        "Information Gain plays a crucial role in decision tree construction by choosing the best attribute that provides maximum reduction in entropy, resulting in an efficient and accurate classification model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fx6adsoopMd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
        "weaknesses, and appropriate use cases.\n",
        "\n",
        "Answer:\n",
        "Gini Impurity and Entropy are **impurity measures** used in **Decision Tree algorithms** to decide how data should be split at each node. Both measure how mixed the classes are in a dataset, but they differ in calculation, interpretation, and usage.\n",
        "\n",
        "### **1. Definition**\n",
        "\n",
        "* **Gini Impurity:**\n",
        "  Measures the probability that a randomly chosen data point would be incorrectly classified if it were randomly labeled according to the class distribution.\n",
        "\n",
        "* **Entropy:**\n",
        "  Measures the amount of uncertainty or randomness in the dataset.\n",
        "\n",
        "\n",
        "### **2. Formula**\n",
        "\n",
        "* **Gini Impurity:**\n",
        "  [\n",
        "  Gini = 1 - \\sum p_i^2\n",
        "  ]\n",
        "\n",
        "* **Entropy:**\n",
        "  [\n",
        "  Entropy = -\\sum p_i \\log_2 p_i\n",
        "  ]\n",
        "\n",
        "Where ( p_i ) is the probability of class ( i ).\n",
        "\n",
        "\n",
        "### **3. Range of Values**\n",
        "\n",
        "* **Gini Impurity:**\n",
        "  Ranges from **0 to 0.5** (for binary classification).\n",
        "  0 means pure node.\n",
        "\n",
        "* **Entropy:**\n",
        "  Ranges from **0 to 1** (for binary classification).\n",
        "  0 means pure node.\n",
        "\n",
        "\n",
        "\n",
        "### **4. Computational Complexity**\n",
        "\n",
        "* **Gini Impurity:**\n",
        "  Faster to compute because it does not involve logarithmic calculations.\n",
        "\n",
        "* **Entropy:**\n",
        "  Slower due to logarithmic operations.\n",
        "\n",
        "\n",
        "\n",
        "### **5. Sensitivity to Class Distribution**\n",
        "\n",
        "* **Gini Impurity:**\n",
        "  Less sensitive to small changes in probability; tends to isolate the most frequent class.\n",
        "\n",
        "* **Entropy:**\n",
        "  More sensitive to changes in class probabilities; gives more balanced splits.\n",
        "\n",
        "\n",
        "\n",
        "### **6. Usage in Algorithms**\n",
        "\n",
        "* **Gini Impurity:**\n",
        "  Used in **CART (Classification and Regression Trees)** algorithm (default in scikit-learn).\n",
        "\n",
        "* **Entropy:**\n",
        "  Used in **ID3 and C4.5** algorithms.\n",
        "\n",
        "\n",
        "\n",
        "### **7. Strengths**\n",
        "\n",
        "* **Gini Impurity:**\n",
        "\n",
        "  * Faster performance\n",
        "  * Suitable for large datasets\n",
        "\n",
        "* **Entropy:**\n",
        "\n",
        "  * Theoretically sound (information theory based)\n",
        "  * Produces informative splits\n",
        "\n",
        "\n",
        "\n",
        "### **8. Weaknesses**\n",
        "\n",
        "* **Gini Impurity:**\n",
        "\n",
        "  * Slightly less informative in some edge cases\n",
        "\n",
        "* **Entropy:**\n",
        "\n",
        "  * Computationally expensive\n",
        "\n",
        "\n",
        "\n",
        "### **9. Use Cases**\n",
        "\n",
        "* Use **Gini Impurity** when speed is important and dataset is large.\n",
        "* Use **Entropy** when interpretability and information-based splitting is preferred.\n",
        "\n"
      ],
      "metadata": {
        "id": "WVyqhmG0YgQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "\n",
        "**Pre-Pruning**, also known as **Early Stopping**, is a technique used in Decision Trees to **control the growth of the tree during the training phase**. In this approach, the tree is stopped from growing further **before it becomes too complex**, even if further splits could improve training accuracy.\n",
        "\n",
        "The main objective of pre-pruning is to **prevent overfitting**, which occurs when a decision tree learns noise and unnecessary details from the training data, resulting in poor performance on unseen data.\n",
        "\n",
        "### **How Pre-Pruning Works**\n",
        "\n",
        "During the construction of a decision tree, each potential split is evaluated. If a split does not satisfy certain predefined conditions, the algorithm **does not perform the split**, and the node becomes a **leaf node**.\n",
        "\n",
        "### **Common Pre-Pruning Criteria**\n",
        "\n",
        "1. **Maximum Depth**: Limit the maximum depth of the tree.\n",
        "\n",
        "2. **Minimum Samples for Split**: A node must have a minimum number of samples to be split.\n",
        "\n",
        "3. **Minimum Information Gain**: Split is allowed only if information gain exceeds a threshold.\n",
        "\n",
        "4. **Minimum Samples in Leaf Node**: Ensures each leaf contains enough data points.\n",
        "\n",
        "5. **Minimum Impurity Decrease**: Split only if impurity reduction is significant.\n",
        "\n",
        "### **Advantages of Pre-Pruning**\n",
        "\n",
        "* Reduces overfitting\n",
        "* Improves generalization ability\n",
        "* Produces simpler and more interpretable trees\n",
        "* Reduces training time and memory usage\n",
        "\n",
        "### **Disadvantages of Pre-Pruning**\n",
        "\n",
        "* May stop tree growth too early\n",
        "* Can lead to underfitting\n",
        "* Requires careful selection of stopping criteria\n",
        "\n",
        "### **Example**\n",
        "\n",
        "If splitting a node results in very small information gain or the number of samples is below a set threshold, the algorithm **stops further splitting**, even if the node is not perfectly pure.\n",
        "\n"
      ],
      "metadata": {
        "id": "LGukpnqnQ3uH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "ui9DHP2pRUU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY6BvDuvo_6n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0a2f45d-8e53-42c3-963c-e85ec2497615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.013333333333333329\n",
            "sepal width (cm): 0.0\n",
            "petal length (cm): 0.5640559581320451\n",
            "petal width (cm): 0.4226107085346215\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data      # Features\n",
        "y = data.target    # Target labels\n",
        "\n",
        "# Create Decision Tree Classifier with Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(data.feature_names, model.feature_importances_):\n",
        "    print(f\"{feature}: {importance}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer:  \n",
        "\n",
        "A **Support Vector Machine (SVM)** is a **supervised machine learning algorithm** used for **classification and regression** problems. Its main objective is to find an **optimal decision boundary (hyperplane)** that separates data points of different classes with the **maximum margin**.\n",
        "\n",
        "### **Working Principle of SVM**\n",
        "\n",
        "SVM works by identifying the **best hyperplane** that divides the data into classes. The data points closest to this hyperplane are called **support vectors**. These support vectors are the most important points because they directly influence the position and orientation of the hyperplane.\n",
        "\n",
        "The margin is the distance between the hyperplane and the nearest data points from each class. SVM aims to **maximize this margin**, which helps in better generalization and reduces overfitting.\n",
        "\n",
        "### **Key Concepts in SVM**\n",
        "\n",
        "1. **Hyperplane**: A decision boundary that separates different classes.\n",
        "\n",
        "2. **Support Vectors**: Data points closest to the hyperplane.\n",
        "\n",
        "3. **Margin**: Distance between the hyperplane and support vectors.\n",
        "\n",
        "4. **Kernel Function**: Used to handle non-linearly separable data by mapping it to a higher-dimensional space.\n",
        "\n",
        "### **Types of Kernels**\n",
        "\n",
        "* **Linear Kernel** – for linearly separable data\n",
        "\n",
        "* **Polynomial Kernel** – for curved boundaries\n",
        "\n",
        "* **Radial Basis Function (RBF)** – most commonly used\n",
        "\n",
        "* **Sigmoid Kernel**\n",
        "\n",
        "### **Advantages of SVM**\n",
        "\n",
        "* Effective in high-dimensional spaces\n",
        "\n",
        "* Works well with small and medium-sized datasets\n",
        "\n",
        "* Robust against overfitting\n",
        "\n",
        "* Can handle non-linear data using kernels\n",
        "\n",
        "### **Disadvantages of SVM**\n",
        "\n",
        "* Computationally expensive for large datasets\n",
        "\n",
        "* Choosing the right kernel and parameters is difficult\n",
        "\n",
        "* Less interpretable compared to decision trees\n",
        "\n",
        "### **Applications of SVM**\n",
        "\n",
        "* Image and face recognition\n",
        "\n",
        "* Text classification and spam detection\n",
        "\n",
        "* Bioinformatics\n",
        "\n",
        "* Handwriting recognition\n",
        "\n"
      ],
      "metadata": {
        "id": "Rfa5VjYwRodf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer:\n",
        "The **Kernel Trick** is a powerful technique used in **Support Vector Machines (SVM)** to handle **non-linearly separable data**. It allows SVM to create **non-linear decision boundaries** without explicitly transforming the data into a higher-dimensional space.\n",
        "\n",
        "### **Why Kernel Trick is Needed**\n",
        "\n",
        "In many real-world problems, data cannot be separated using a straight line (or linear hyperplane). To separate such data, it must be mapped to a **higher-dimensional space** where it becomes linearly separable.\n",
        "The Kernel Trick performs this mapping **implicitly**, making computation efficient.\n",
        "\n",
        "### **How Kernel Trick Works**\n",
        "\n",
        "Instead of computing the transformation of data points into higher dimensions, the kernel function computes the **inner product** of data points directly in the transformed space.\n",
        "This avoids heavy computations and reduces processing time.\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "K(xi​,xj​)=ϕ(xi​)⋅ϕ(xj​)\n",
        "\n",
        "where:\n",
        "\n",
        "*ϕ is a mapping function to higher dimensions\n",
        "\n",
        "K is the kernel function\n",
        "\n",
        "### **Common Kernel Functions**\n",
        "\n",
        "1. **Linear Kernel**\n",
        "   ( K(x_i, x_j) = x_i . x_j )\n",
        "   Used when data is linearly separable.\n",
        "\n",
        "2. **Polynomial Kernel**\n",
        "   ( K(x_i, x_j) = (x_i . x_j + c)^d )\n",
        "   Used for curved decision boundaries.\n",
        "\n",
        "3. **Radial Basis Function (RBF) Kernel**\n",
        "   ( K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2) )\n",
        "   Most popular kernel for complex data.\n",
        "\n",
        "4. **Sigmoid Kernel**\n",
        "   ( K(x_i, x_j) = \\tanh(\\alpha x_i \\cdot x_j + c) )\n",
        "\n",
        "### **Advantages of Kernel Trick**\n",
        "\n",
        "* Handles complex, non-linear data\n",
        "* Avoids explicit high-dimensional transformation\n",
        "* Computationally efficient\n",
        "* Improves model flexibility\n",
        "\n",
        "### **Limitations**\n",
        "\n",
        "* Choosing the right kernel is challenging\n",
        "* Kernel selection affects performance\n",
        "* Risk of overfitting with complex kernels\n",
        "\n",
        "### **Example**\n",
        "\n",
        "If data is circular in shape and cannot be separated linearly, the **RBF kernel** maps it to higher dimensions where a linear hyperplane can separate it.\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "The Kernel Trick enables SVM to solve non-linear classification problems efficiently by implicitly mapping data to higher-dimensional space using kernel functions.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cA1_buK-SDPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "Xuuo5lUWSvdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print the accuracies\n",
        "print(\"Accuracy of SVM with Linear Kernel:\", accuracy_linear)\n",
        "print(\"Accuracy of SVM with RBF Kernel:\", accuracy_rbf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SX0QBx9Aq6gB",
        "outputId": "32531383-a109-4213-aa9e-1e7caa790d63"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM with Linear Kernel: 0.9814814814814815\n",
            "Accuracy of SVM with RBF Kernel: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "Answer:\n",
        "The **Naïve Bayes classifier** is a **supervised machine learning algorithm** used for **classification problems**. It is a **probabilistic classifier** based on **Bayes’ Theorem**, which calculates the probability of a class label given a set of input features. The classifier assigns a data point to the class with the **highest posterior probability**.\n",
        "\n",
        "\n",
        "\n",
        "### Bayes’ Theorem\n",
        "\n",
        "Naïve Bayes uses the following formula:\n",
        "\n",
        "P(C∣X)=P(X∣C)P(C)/ P(X)​\n",
        "\n",
        "Where:\n",
        "\n",
        "* ( P(C|X) ) = Posterior probability of class ( C ) given features ( X )\n",
        "* ( P(X|C) ) = Likelihood of features given class\n",
        "* ( P(C) ) = Prior probability of class\n",
        "* ( P(X) ) = Evidence (constant for all classes)\n",
        "\n",
        "\n",
        "\n",
        "### Why is it called **“Naïve”**?\n",
        "\n",
        "The classifier is called **“Naïve”** because it assumes that **all input features are conditionally independent of each other given the class label**. This means the presence or absence of one feature does not affect the presence of another feature.\n",
        "\n",
        "This assumption is **rarely true in real-world data**, hence the term *naïve*. However, even with this unrealistic assumption, the algorithm performs very well in many practical applications.\n",
        "\n",
        "\n",
        "\n",
        "### Types of Naïve Bayes Classifiers\n",
        "\n",
        "1. **Gaussian Naïve Bayes** – Used for continuous data\n",
        "2. **Multinomial Naïve Bayes** – Used for text data and word counts\n",
        "3. **Bernoulli Naïve Bayes** – Used for binary features\n",
        "\n",
        "\n",
        "\n",
        "### Advantages\n",
        "\n",
        "* Simple and easy to understand\n",
        "* Very fast training and prediction\n",
        "* Works well with high-dimensional data\n",
        "* Requires less training data\n",
        "* Effective for text classification problems\n",
        "\n",
        "\n",
        "\n",
        "### Limitations\n",
        "\n",
        "* Strong independence assumption\n",
        "* Less accurate when features are highly correlated\n",
        "* Probability estimates may be poor\n",
        "\n",
        "\n",
        "\n",
        "### Applications\n",
        "\n",
        "* Spam email detection\n",
        "* Sentiment analysis\n",
        "* Document classification\n",
        "* Medical diagnosis\n",
        "\n",
        "\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The **Naïve Bayes classifier** is called *naïve* due to its **feature independence assumption**, but it remains a **powerful, efficient, and widely used classification algorithm**, especially in text-based and large-scale applications.\n"
      ],
      "metadata": {
        "id": "ew_PK5-HrP1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "Answer:\n",
        "Naïve Bayes is a **probabilistic classification algorithm** based on **Bayes’ Theorem**. Depending on the **type of data and feature distribution**, different variants of Naïve Bayes are used. The three most common types are **Gaussian Naïve Bayes**, **Multinomial Naïve Bayes**, and **Bernoulli Naïve Bayes**.\n",
        "\n",
        "\n",
        "\n",
        "## 1. Gaussian Naïve Bayes\n",
        "\n",
        "**Gaussian Naïve Bayes** is used when the input features are **continuous numerical values** and are assumed to follow a **normal (Gaussian) distribution**.\n",
        "\n",
        "### Key Characteristics:\n",
        "\n",
        "* Assumes features follow a **Gaussian distribution**\n",
        "* Uses **mean and variance** for probability estimation\n",
        "* Suitable for **real-valued data**\n",
        "\n",
        "### Example Applications:\n",
        "\n",
        "* Medical diagnosis (blood pressure, temperature)\n",
        "* Iris dataset classification\n",
        "\n",
        "\n",
        "\n",
        "## 2. Multinomial Naïve Bayes\n",
        "\n",
        "**Multinomial Naïve Bayes** is mainly used for **discrete count data**, especially in **text classification** problems.\n",
        "\n",
        "### Key Characteristics:\n",
        "\n",
        "* Works with **frequency/count of features**\n",
        "* Commonly used with **Bag-of-Words** or **TF-IDF**\n",
        "* Feature values represent how often something occurs\n",
        "\n",
        "### Example Applications:\n",
        "\n",
        "* Spam detection\n",
        "* News article classification\n",
        "* Sentiment analysis\n",
        "\n",
        "\n",
        "\n",
        "## 3. Bernoulli Naïve Bayes\n",
        "\n",
        "**Bernoulli Naïve Bayes** is used when features are **binary (0 or 1)**, indicating the **presence or absence** of a feature.\n",
        "\n",
        "### Key Characteristics:\n",
        "\n",
        "* Features take **binary values**\n",
        "* Considers both presence and absence of features\n",
        "* Suitable for binary feature vectors\n",
        "\n",
        "### Example Applications:\n",
        "\n",
        "* Text classification with binary word occurrence\n",
        "* Email spam detection (word present or not)\n",
        "\n",
        "\n",
        "\n",
        "## Comparison Table\n",
        "\n",
        "| Feature              | Gaussian NB       | Multinomial NB      | Bernoulli NB    |\n",
        "| -------------------- | ----------------- | ------------------- | --------------- |\n",
        "| Data Type            | Continuous        | Discrete counts     | Binary          |\n",
        "| Distribution Assumed | Normal (Gaussian) | Multinomial         | Bernoulli       |\n",
        "| Feature Values       | Real numbers      | Integer counts      | 0 or 1          |\n",
        "| Common Use Case      | Numerical data    | Text frequency data | Binary features |\n",
        "| Example              | Medical data      | Spam filtering      | Word presence   |\n",
        "\n",
        "\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "* **Gaussian Naïve Bayes** is best for **continuous numerical data**.\n",
        "* **Multinomial Naïve Bayes** is ideal for **text and count-based data**.\n",
        "* **Bernoulli Naïve Bayes** is suitable for **binary feature data**.\n",
        "\n",
        "Choosing the correct variant depends on the **nature of the dataset and feature representation**.\n"
      ],
      "metadata": {
        "id": "-YhFhTLOsNkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "6eEGUwtCtSFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy of Gaussian Naïve Bayes classifier:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A2ScoLatqAG",
        "outputId": "273c4cc7-4772-4955-9b72-fe300ce1e500"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes classifier: 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}